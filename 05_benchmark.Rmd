---
title: "Benchmarking"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r, echo = FALSE}
library(ampir)
library(caret)
library(tidyverse)
library(pROC)
```


When benchmarking the performance of AMP predictors a number of important factors need to be considered:

1. Any benchmark dataset will likely include some AMPs used for training in one or more of the predictors. Since most predictors are not open source they are provided as-is and it is almost impossible to devise a completely impartial benchmark based on AMPs that were not used to train any of the predictors.
2. An existing benchmark dataset provided by [Xiao et al. 2013](https://doi.org/10.1016/j.ab.2013.01.019) has been adopted by several subsequent authors. The composition of this dataset is geared toward a mature peptide test because the positive cases are all mature peptides. The background dataset is perhaps not ideal to represent a background of mature (non-AMP) peptides because it has a length distribution suggesting that it contains a significant fraction of full length precursors.
3. A realistic test of AMP prediction in genome-wide scans should use a benchmark dataset that is highly unbalanced, just as a real genome protein set would be. For example in the *Arabidopsis* genome AMPs make up less than 1% of proteins.  
4. Real genomes contain non-AMP proteins that may resemble AMPs in some ways (e.g. secreted proteins, transmembrane proteins) and which will therefore make the classification problem more difficult. Any benchmark that does not include these proteins will most likely provide inflated estimates of accuracy.

In light of these issues we tested the performance of `ampir` against contemporary AMP predictors using several benchmark datasets:

1. The [Xiao et al. 2013](https://doi.org/10.1016/j.ab.2013.01.019) benchmark dataset. This was included in the interest of consistency with benchmarking from previous work but results from this benchmark are not likely to reflect real-world performance.
2. A subset of the `ampir` mature peptide training data which we set aside for model evaluation and was not used in training. This dataset consists of known AMP mature peptides as positive cases and non-AMP mature peptides as negative cases. It should reflect real-world performance in situations where a researcher has access to a mature peptide sequence (e.g. by mass spectrometry) and wishes to determine if it is an AMP or another type of small peptide such as a toxin or neuropeptide.
3. A whole-genome scanning benchmark for species with the best available annotated AMP repertoires. We chose an animal (Human) and a plant (*Arabidopsis thaliana*) for this test. 

**Table 5.1:** AMP predictors with their papers and model accessiblity

| AMP predictor name | Reference | Availability |
| ------------------------ | --------------- | -------------- |
| AMP scanner v2 | [Veltri et al. 2018](https://doi.org/10.1093/bioinformatics/bty179) | [amp scanner webserver](https://www.dveltri.com/ascan/v2/ascan.html) |
| amPEP  | [Bhadra et al. 2018](https://doi.org/10.1038/s41598-018-19752-w) | [MATLAB source code](https://sourceforge.net/projects/axpep/files/AmPEP_MATLAB_code/)
| iAMPpred | [Meher et al. 2017](https://doi.org/10.1038/srep42362) | [iAMPpred webserver](http://cabgrid.res.in:8080/amppred/)

AMP predictors were accessed in ***April 2020***

### Mature peptide benchmarks

Both the [Xiao et al. 2013](https://doi.org/10.1016/j.ab.2013.01.019) benchmark and the `ampir` testing set are focussed on mature peptide prediction (since mature peptides form the bulk of positive cases). The benchmarks differ most substantially in the composition of their background datasets. The Xiao et al. background data has a peak in length distribution around 80-90 AA whereas for the `ampir` test set this is more similar to the target set at around 40 AA.

The plot below shows performance of all predictors in the form of receiver operating characteristic (ROC) curves. These show the tradeoff between false positive rate and true positive rate (also called recall). A few points to note from this plot:

- The `ampir_mature` model performs well on both datasets whereas the `ampir_precursor` model performs very poorly. Users of `ampir` should therefore take care to always select the appropriate model for their task depending on the nature of the input data (mature peptides or precursor proteins).
- Some predictors do not perform well at the extremes of the ROC curve. This reflects the ability of the predictor to produce accurate probability values across the full range of probabilities. In the case of `ampscanner v2` for example we see that its curve does not extend into the low false positive regime.  This is because its probability distribution is strongly concentrated at the extremes (0 and 1), and a relatively large number of non-AMP peptides have been assigned a probability of 1. 
- The best performing predictors in the low false positive regime are `ampep` and `ampir_mature` in both datasets.

```{r}
calc_cm_metrics <- function(p_threshold, df) {
  
  TP <- df %>% filter((actual=="Tg")) %>% filter(prob_AMP >= p_threshold) %>% n_distinct()
  FP <- df %>% filter((actual=="Bg")) %>% filter(prob_AMP >= p_threshold) %>% n_distinct()
  TN <- df %>% filter((actual=="Bg")) %>% filter(prob_AMP < p_threshold) %>% n_distinct()
  FN <- df %>% filter((actual=="Tg")) %>% filter(prob_AMP < p_threshold) %>% n_distinct()
  
  Specificity <- round(TN / (TN + FP), digits = 5) #aka TNR
  Recall <- round(TP / (TP + FN), digits = 5) # aka sensitivity, TPR
  Precision <- round(TP/ (TP + FP), digits = 5) # positive predictive value
  FPR <- FP / (TN + FP)
  
  cm <- c(TP, FP, TN, FN, Specificity, Recall, Precision, FPR, p_threshold)
  names(cm) <-c("TP", "FP", "TN", "FN", "Specificity", "Recall", "Precision", "FPR", "p_threshold") 
  cm
}

compress_model <- function(model){
  model$trainingData <- model$trainingData[1,]
  model$resampledCM <- NULL
  model$control <- NULL
  model
}
```



```{r, echo = FALSE}
# ampir
#
precursor_model <- compress_model(readRDS("raw_data/ampir_models/tuned_precursor_imbal.rds"))
mature_model <- compress_model(readRDS("raw_data/ampir_models/tuned_mature.rds"))

benchmark_fasta_files <- c("raw_data/benchmarking/datasets/iamp2l/iamp2l_bench.fasta",
                           "raw_data/benchmarking/datasets/ampir/mature_eval.fasta",
                           "raw_data/benchmarking/datasets/ampir/precursor_eval.fasta",
                           "raw_data/benchmarking/datasets/ampir/mature_train.fasta",
                           "raw_data/benchmarking/datasets/ampir/precursor_train.fasta")

ampir_benchmark_cachenames <- c("cache/xbench_ampir_raw.rds",
                                "cache/ampir_bench_raw.rds",
                                "cache/ampir_prec_bench_raw.rds",
                                "cache/ampir_train_raw.rds",
                                "cache/ampir_prec_train_raw.rds")

ampir_benchmark_info <- data.frame(fasta=benchmark_fasta_files,cache=ampir_benchmark_cachenames,
                                   name=c("Xiao","mature_eval","precursor_eval","mature_train","precursor_train"),
                                   stringsAsFactors = FALSE)

benchmark_faa_data <- lapply(ampir_benchmark_info$fasta,read_faa)

predict_bench <- function(cachename,benchdata){
  if ( file.exists(cachename) ){
    xbench_ampir_raw <- read_rds(cachename)
  } else {
    xbench_ampir_raw_prec <- predict_amps(benchdata,n_cores = 4, model = precursor_model) %>% add_column(model = "ampir_precursor")
    xbench_ampir_raw_mat <- predict_amps(benchdata,n_cores = 4, model = mature_model) %>% add_column(model = "ampir_mature")
    xbench_ampir_raw <- rbind(xbench_ampir_raw_mat,xbench_ampir_raw_prec)
    write_rds(xbench_ampir_raw,cachename)
  } 
  xbench_ampir_raw
}

raw_bench_results <- lapply(1:nrow(ampir_benchmark_info),function(i){  predict_bench(ampir_benchmark_info$cache[i], benchmark_faa_data[[i]]) })
names(raw_bench_results) <- ampir_benchmark_info$name

get_pattern <- function(data){
  if ( sum(grepl(data$seq_name,pattern="_Tg$"))==0 ){
    pat="^AP"
  } else {
    pat="_Tg$"
  }
  return(pat)
}

annotate_classes <- function(raw_bench){
  pat <- get_pattern(raw_bench)
  raw_bench %>% 
    mutate(actual = ifelse(grepl(seq_name,pattern = pat), "Tg", "Bg")) %>% 
    mutate(predicted = ifelse(prob_AMP>0.5, "Tg","Bg")) %>%
    mutate(actual = factor(actual), predicted = factor(predicted))
}

ampir_bench_raw <- lapply(raw_bench_results,annotate_classes)

make_roc_curve <- function(anno_bench_results,name){
  roc <- do.call(rbind,lapply(c("ampir_precursor","ampir_mature"),function(mdl){
    as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, anno_bench_results %>% filter(model==mdl)))) %>%
    add_column(model = mdl)
  })) %>% 
    add_column(benchmark = name)
  roc
}

ampir_rocs <- lapply(names(ampir_bench_raw),function(n){ make_roc_curve(ampir_bench_raw[[n]],n) } )
ampir_rocs_long <- do.call(rbind,ampir_rocs)
```



```{r}
ampep_result_files <- c("raw_data/benchmarking/results/ampep/ampep_iamp2l_bench.txt",
                           "raw_data/benchmarking/results/ampep/mature_eval_ampep.txt",
                           "raw_data/benchmarking/results/ampep/precursor_eval_ampep.txt",
                           "raw_data/benchmarking/results/ampep/precursor_train_ampep.txt",
                           "raw_data/benchmarking/results/ampep/mature_train_ampep.txt")
names(ampep_result_files) <- c("Xiao","mature_eval","precursor_eval","precursor_train","mature_train")

annotate_classes_ampep <- function(raw_bench){
  pat <- get_pattern(raw_bench)
  raw_bench %>% 
    mutate(actual = ifelse(grepl(seq_name,pattern = pat), "Tg", "Bg")) %>% 
    mutate(predicted = ifelse(prediction==1, "Tg","Bg")) %>%
    mutate(actual = factor(actual), predicted = factor(predicted))
}

make_roc_curve_ampep <- function(xbench_ampep,name){
  as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_ampep))) %>%
  add_column(model = "ampep") %>% add_column(benchmark = name)
}

ampep_bench_raw <- lapply(names(ampep_result_files),function(n) { 
  read_csv(ampep_result_files[n]) %>% 
    rename(seq_name=Row) %>% 
    rename(prob_AMP = score) %>% 
    annotate_classes_ampep()})
names(ampep_bench_raw) <- names(ampep_result_files)

ampep_rocs <- lapply(names(ampep_bench_raw), function(n) { make_roc_curve_ampep(ampep_bench_raw[[n]],n)})
ampep_rocs_long <- do.call(rbind,ampep_rocs)
```

```{r}
# ampscannerv2

ampscanv2_result_files <- c("raw_data/benchmarking/results/ampscanv2/iamp2l/1585811335833_Prediction_Summary.csv",
                            "raw_data/benchmarking/results/ampscanv2/ampir/1588756293516_Prediction_Summary.csv",
                            "raw_data/benchmarking/results/ampscanv2/ampir_prec/1588756490979_Prediction_Summary.csv",
                            "raw_data/benchmarking/results/ampscanv2/ampir/1592431117418_Prediction_Summary.csv",
                            "raw_data/benchmarking/results/ampscanv2/ampir_prec/1592432311616_Prediction_Summary.csv"
                            )

names(ampscanv2_result_files) <- c("Xiao","mature_eval","precursor_eval","mature_train","precursor_train")

annotate_classes_ampscanv2 <- function(raw_bench){

  pat <- get_pattern(raw_bench)
  
  
  raw_bench %>% mutate(actual = ifelse(grepl(seq_name,pattern = pat), "Tg", "Bg")) %>% 
  rename(prob_AMP = Prediction_Probability) %>%
  mutate(predicted = ifelse(Prediction_Class == "AMP", "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))
}

ampscanv2_bench_raw <- lapply(names(ampscanv2_result_files),function(n){
  read_csv(ampscanv2_result_files[n]) %>% 
    rename(seq_name=SeqID) %>% 
    annotate_classes_ampscanv2()
})
names(ampscanv2_bench_raw) <- names(ampscanv2_result_files)

make_roc_curve_ampscanv2 <- function(xbench_ampscanv2,name){
  as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_ampscanv2))) %>%
  add_column(model = "ampscannerv2") %>% add_column(benchmark = name)
}

ampscanv2_rocs <- lapply(names(ampscanv2_bench_raw), function(n){ make_roc_curve_ampscanv2(ampscanv2_bench_raw[[n]],n)})
ampscanv2_rocs_long <- do.call(rbind,ampscanv2_rocs)
```


```{r}
iampred_result_files <- c("raw_data/benchmarking/results/iamppred/iamp2l_bench.csv",
                          "raw_data/benchmarking/results/iamppred/ampir_mature.csv",
                          "raw_data/benchmarking/results/iamppred/ampir_precursor.csv")
names(iampred_result_files) <- c("Xiao","mature_eval","precursor_eval")

annotate_classes_iampred <- function(raw_bench){
  pat <- get_pattern(raw_bench)

  raw_bench %>% mutate(actual = ifelse(grepl(seq_name,pattern = pat), "Tg", "Bg")) %>% 
  mutate(prob_AMP = pmax(antibacterial,antiviral,antifungal)) %>%
  mutate(predicted = ifelse(prob_AMP >= 0.5, "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))
}

iampred_bench_raw <- lapply(names(iampred_result_files),function(n){
  if(n=="Xiao"){
    readfunc=read_csv
    } else {
    readfunc=read_tsv
    }
  readfunc(iampred_result_files[n]) %>% 
    rename(seq_name=name_fasta) %>% 
    annotate_classes_iampred()
})
names(iampred_bench_raw) <- names(iampred_result_files)

make_roc_curve_iampred <- function(xbench_iampred,name){
  as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_iampred))) %>%
  add_column(model = "iamppred") %>% add_column(benchmark = name)
}

iampred_rocs <- lapply(names(iampred_bench_raw), function(n){ make_roc_curve_iampred(iampred_bench_raw[[n]],n)})
iampred_rocs_long <- do.call(rbind,iampred_rocs)
```


```{r}
models_roc_mature <- rbind(ampir_rocs_long,ampep_rocs_long,ampscanv2_rocs_long,
                    iampred_rocs_long)

models_roc_mature %>% 
  filter(benchmark %in% c("Xiao","mature_eval")) %>% 
  mutate(mpbench = ifelse(benchmark=="Xiao","Xiao","ampir mature peptide")) %>% 
  ggplot() + 
  geom_line(aes(x = FPR, y = Recall, colour = model)) + 
  xlim(0,1) + facet_wrap(~mpbench) +
  labs(x = "False positive rate", y = "True positive rate")
```

**Figure 5.1:** Performance of a range of AMP predictors against two mature peptide testing datasets.

### Real Genome Benchmark

Since we are building a model for the purpose of genome-wide prediction a realistic test must involve data with composition similar to that of a complete proteome. 

One approach is to use whole genomes that have been well annotated for AMPs. Here we chose the Human and *Arabidopsis* genomes because these represent phylogenetically distinct lineages (animals and plants), and their genomes are among the best annotated for AMPs. A few other points to note about this test are:

- We were able to run this test for `ampir`, `ampep` and `ampscanner v2` only because other predictors were unable to handle the large number of candidates sequences (~100k) in a practical manner. 
- We used a specially generated model for `ampir` that was trained without Human or *Arabidopsis* proteins to avoid any potential for inflated accuracy estimates in this test. It should be noted that other predictors would have no such restriction.
- The actual number of true positives is probably an underestimate since our knowledge of the AMP repertoires of both species is incomplete.

```{r}
human_proteome <- readxl::read_excel("raw_data/benchmarking/datasets/human/uniprot-proteome_UP000005640.xlsx",guess_max = 10000)
arath_proteome <- readxl::read_excel("raw_data/benchmarking/datasets/arath/uniprot-proteome_up000006548.xlsx",guess_max = 40000)

reference_proteomes <- rbind(human_proteome, arath_proteome) %>% 
  mutate(actual = ifelse(grepl(`Keyword ID`,pattern="KW-0929"),"Tg","Bg"))
```


```{r}
# ampir

ref_df <- reference_proteomes %>% select(seq_name=Entry,seq_aa=Sequence)

if ( file.exists("cache/ref_predictions_ampir.rds")){
  ref_predictions_ampir <- read_rds("cache/ref_predictions_ampir.rds")
} else {
  ampir_genome_model <- compress_model(readRDS("raw_data/ampir_models/tuned_precursor_imbal_nobench.rds"))

  ref_predictions_ampir_prec <- predict_amps(as.data.frame(ref_df), n_cores=4, model = ampir_genome_model) %>% add_column(method="ampir_precursor")
  ref_predictions_ampir_mature <- predict_amps(as.data.frame(ref_df), n_cores=4, model = mature_model) %>% add_column(method="ampir_mature")
  
  ref_predictions_ampir <- rbind(ref_predictions_ampir_prec,ref_predictions_ampir_mature)
  
  write_rds(ref_predictions_ampir,"cache/ref_predictions_ampir.rds")
}


ampir_genome_bench <- reference_proteomes %>% left_join(ref_predictions_ampir %>% select(-seq_aa),by=c("Entry"="seq_name")) %>% 
  filter(!is.na(prob_AMP)) %>% 
  select(ID=Entry,prob_AMP,Organism,actual,method) 

organisms = c("Homo sapiens (Human)","Arabidopsis thaliana (Mouse-ear cress)")

get_genome_roc <- function(data, name){
  do.call(rbind,lapply(organisms,function(org){ 
    as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics , data %>% filter(Organism==org)))) %>%
    add_column(organism=org)
  })) %>%   
  add_column(model = name)
}

if ( file.exists("cache/ampir_genome_roc.rds")){
  ampir_genome_roc <- read_rds("cache/ampir_genome_roc.rds")
} else {
  ampir_genome_roc <- do.call(rbind,lapply(c("ampir_precursor","ampir_mature"),function(meth){
    get_genome_roc(ampir_genome_bench %>% filter(method==meth),meth)
  }))
  write_rds(ampir_genome_roc,"cache/ampir_genome_roc.rds")
}
```



```{r}
ampscan_files <- c(list.files("raw_data/benchmarking/results/ampscanv2/arath/", pattern="*.csv",full.names = T),
                   list.files("raw_data/benchmarking/results/ampscanv2/human/", pattern="*.csv",full.names = T))

ampscan_genome_bench <- do.call(rbind,lapply(ampscan_files,read_csv)) %>% 
  separate(SeqID,into = c("database","Entry","Entry name"),sep = "\\|") %>% 
  left_join(reference_proteomes,by="Entry") %>% 
  select(ID=Entry,prob_AMP=Prediction_Probability,Organism,actual) %>% 
  add_column(method="ampscannerv2")

ampscan_genome_roc <- get_genome_roc(ampscan_genome_bench,"ampscannerv2")
```


```{r}
ampep_files <- c("raw_data/benchmarking/results/ampep//arath_ampep.txt","raw_data/benchmarking/results/ampep//human_ampep.txt")

ampep_genome_bench <- do.call(rbind,lapply(ampep_files,read_csv)) %>% 
  separate(Row,into = c("database","Entry","Entry name"),sep = "\\|") %>% 
  left_join(reference_proteomes,by="Entry") %>% 
  select(ID=Entry,prob_AMP=score,Organism,actual) %>% 
  add_column(method="ampep")

ampep_genome_roc <- get_genome_roc(ampep_genome_bench,"ampep")
```

```{r}
library(cowplot)
genome_rocs <- rbind(ampir_genome_roc,ampscan_genome_roc,ampep_genome_roc)

fpr_plot <- ggplot(genome_rocs) + 
  geom_line(aes(x = FPR, y = Recall, colour = model)) + 
  xlim(0,1) +
  facet_wrap(~organism) +
  labs(x= "False positive rate", y = "True positive rate") + theme(legend.position = "bottom", legend.title = element_blank())

sr_plot <- ggplot(genome_rocs) + 
  geom_line(aes(x = Precision, y = Recall, colour = model)) + 
  facet_wrap(~organism) +
  labs(x= "Precision", y = "Sensitivity") + theme(legend.position = "none")

plot_grid(sr_plot,fpr_plot, ncol = 1, rel_heights = c(1,1.5))

ggsave("figures/genome_rocs.png")
```


**Figure 5.2:** Performance of various AMP predictors in classifying whole proteome data for Human and *Arabidopsis*. Performance is shown as a balance of sensitivity and precision (top row) and as a ROC curve (second row). 

In Figure 5.2 (first row) we show how various predictors perform on genome-scanning data as a tradeoff between sensitivity (proportion of AMPs detected among all AMPs present in the dataset) and precision (proportion of true positives compared with all positive results). This shows that in order to obtain a high precision it is necessary to discard many true AMPs (low sensitivity). Note that the `ampir_precursor` model clearly outperforms all other models in this context.

In Figure 5.2 (second row) we show ROC curves for all predictors on the Human and *Arabidopsis* data but it is important to remember that in this context the low false positive regime is especially important. This is because of the extremely low frequency of true positives in the data (less than 1%). This is explored further in Figure 5.3 but for now it is important to note that `ampscanner v2` is not shown in Figure 5.3 because its ROC curve does not extend into this important regime despite the fact that it otherwise appears to perform very well. `AmPEP` and `ampir_mature` both perform very poorly reflecting the emphasis of their training data on mature peptides rather than precursor proteins.

In order to properly capture the real-world performance of predictors on genome scans we use a plot that emphasises the absolute numbers of true and false positives. On this measure (shown in Figure 5.3) it can be seen that genome-wide prediction of AMPs is still an imperfectly solved problem. Although the `ampir` precursor model clearly performs far better than any other predictors, none were able to predict more than 50% of true AMPs while controlling false positives to under 500. Nevertheless, given the difficulties in identifying AMPs and the importance of this task this level of enrichment is of great practical use, reducing the number of false experimental leads per true positive from many thousands down to tens or hundreds. 

```{r, fig.width=8, fig.height=3}
library(RColorBrewer)
models <- c("ampep","ampir_mature","ampir_precursor","ampscannerv2","iamppred")
model_colors <- brewer.pal(5,"Set1")
names(model_colors) <- models
model_names <- c("amPEP","ampir mature","ampir precursor","AMP Scanner v2","iAMPPred")
names(model_names) <- models

library(ggpubr)
pg <- genome_rocs %>% mutate(organism = ifelse(organism=="Homo sapiens (Human)","Human Genome","A. thaliana Genome")) %>% 
  ggplot() + 
  geom_line(aes(x = FP, y = TP, colour = model),size=1.1) + 
  xlim(0,500) +
  scale_color_manual(values = model_colors) +
  labs(x = "False Positives", y = "True Positives") +
  facet_wrap(~organism, scales = "free_y", nrow = 1) + 
  theme_pubr() + theme(legend.position = "None") +
  theme(strip.background = element_blank(), strip.placement = "outside", strip.text = element_text(size = 10)) +
  theme(axis.text = element_text(size=8))

pp <- ggplot(models_roc_mature %>% filter(benchmark=="mature_eval")) + 
  geom_line(aes(x = FPR, y = Recall, colour = model),size=1.1) + 
  labs(x= "False positive rate", y = "True positive rate") +
  scale_color_manual(values = model_colors, labels = model_names, name="") +
    xlim(0,1) +
  theme_pubr() + theme(legend.position = "None") + ggtitle("Mature Peptides") +
  theme(plot.title = element_text(size=10, hjust = 0.5, vjust = -2))+
  theme(axis.text = element_text(size=10))

legend_b <- get_legend(
  pp + 
    guides(color = guide_legend(nrow = 1)) +
    theme(legend.position = "bottom")
)

library(cowplot)
prow <- plot_grid(pg,pp, rel_widths = c(2,1))

prow2 <- annotate_figure(prow, fig.lab = "A", top = "B")
prow3 <- annotate_figure(prow2, fig.lab = "C", fig.lab.pos = "top.right")

prow4 <- plot_grid(prow3,legend_b, ncol = 1,rel_heights = c(1,0.1))

prow4

ggsave(plot = prow4, filename = "figures/benchmarks.png", width = 178,units = "mm", height = 80,dpi = 600)
ggsave("figures/benchmarks.eps", width = 178,units = "mm", height = 80,dpi = 600)
```

**Figure 5.3:** Performance of `ampir` compared with three existing AMP prediction models iAMPpred (Meher et al., 2017), AmPEP (Bhadra et al., 2018), AMP Scanner (Veltri et al., 2018). Results for iAMPpred are not shown for parts A and B because it was impractical to run on large numbers of sequences. Parts A and B are scaled so that the limits of the y-axis show the full complement of known AMPs in each genome (291 for *Arabidopsis*, 101 for Human), and the limits of the x-axis are restricted to emphasise behaviour in the low false positive (FP) regime (FP < 500) because this is most relevant in whole genome scans. Part C is a receiver operating characteristic (ROC) curve based on the `ampir_mature` reserved testing data. 


## Performance Statistics

```{r}
calc_performance_stats <- function(test_results){
    x_cm_prec <- confusionMatrix(factor(test_results$predicted), factor(test_results$actual), positive = "Tg")
  
  xrc_prec <- roc(test_results$actual, test_results$prob_AMP) 
  c(x_cm_prec$byClass,AUC=as.numeric(xrc_prec$auc))
}

get_bench_list <- function(bench_name){
    lall <- list(ampir_mature=ampir_bench_raw[[bench_name]] %>% filter(model=="ampir_mature"),
                    ampir_precursor=ampir_bench_raw[[bench_name]] %>% filter(model=="ampir_precursor"),
                    ampscannerv2=ampscanv2_bench_raw[[bench_name]],
                    ampep=ampep_bench_raw[[bench_name]])

    if ( bench_name %in% names(iampred_bench_raw) ){
      lall[["iampred"]] <- iampred_bench_raw[[bench_name]]
    }
  lall
}

get_bench_stats <- function(bench_name){
  bench_data <- get_bench_list(bench_name)
  sapply(bench_data,calc_performance_stats) %>% as.data.frame() %>% 
    add_column(dataset=bench_name)
}
```

**Table 5.2:** Model performance on Xiao et al. benchmark dataset
```{r}
knitr::kable(get_bench_stats("Xiao") %>% select(-dataset), digits = 2)
```

**Table 5.3:** Model performance on `ampir_mature` test set
```{r}
knitr::kable(get_bench_stats("mature_eval") %>% select(-dataset), digits = 2)
```

**Table 5.4:** Model performance on `ampir_precursor` test set
```{r}
knitr::kable(get_bench_stats("precursor_eval") %>% select(-dataset), digits = 2)
```

**Table 5.5:** Model performance on the ampir mature __training__ data

```{r}
knitr::kable(get_bench_stats("mature_train") %>% select(-dataset), digits = 2)
```

**Table 5.6:** Model performance on the ampir precursor __training__ data

```{r}
knitr::kable(get_bench_stats("precursor_train") %>% select(-dataset), digits = 2)
```



## Running time

Benchmarking the computational speed of AMP predictors is difficult to do in a completely objective fashion. This is because many predictors are available only through web servers, in which case the performance depends on unknown factors such as server load and configuration. Here we present some approximate benchmarks for the speed of `ampir`, `ampscanner v2` and `ampep` based on a complete Human proteome dataset (74,811 proteins). Performance of `iamppred` could not be evaluated with this dataset as we found that it was unable to handle large numbers of input sequences.

*ampscanner v2*: Maximum file upload size is 50Mb so it was necessary to divide the job into two parts. A stopwatch was used to measure runtime. Timing was started after the upload step had finished so as not to include internet connectivity speed in the test. The reported run time is the sum of runtimes for both parts of the dataset.

*ampep*: `ampep` was run using MATLAB R2019a on an Intel Xeon processor with 40 CPUs but runtime reflects single core performance since ampep did not appear to have a multi-core capability. 


*ampir*: `ampir` was run using the same Intel Xeon processor as was used for the `ampep` benchmark. Since `ampir` is capable of multicore operation we measured its runtime as a function of core-count (see Figure 5.4). In Table 5.7 the runtime of `ampir` with a single core is shown. `ampir` provides comparable performance to `ampscanner v2` when run with 4 cores.

**Table 5.7:** Run time performance on the Human Proteome

Program     | Number of Cores | Runtime (s)
----------- | --------------- | --------------
ampscan v2 | Unknown |  195
ampep | 1 | 1223
ampir | 1 | 469
ampir | 4 | 218
ampir | 8 | 110

```{r}
ampir_human_bench_runtimes <- data.frame(cores=c(1,2,4,8,16,32), time=c(469,323,218,110,69,47))

ampir_human_runtime_plot <- ggplot(ampir_human_bench_runtimes,aes(x=cores,y=time)) + geom_line() + geom_point() + xlab("Number of cores") + ylab("Elapsed time (s)")

ampir_human_runtime_plot

ggsave("figures/ampir_runtime.png")
```

**Figure 5.4** Performance of ampir as a function of core count when running `predict_amps()` on a dataset of 77,000 proteins. 












