---
title: "Benchmarking"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r, echo = FALSE}
library(ampir)
library(caret)
library(tidyverse)
library(pROC)
```


When benchmarking the performance of AMP predictors a number of important factors need to be considered:

1. Any benchmark dataset will likely include some AMPs used for training in one or more of the predictors.  Since most predictors are not open source they are provided as-is and it is almost impossible to devise a completely impartial benchmark based on AMPs that were not used to train any of the predictors.
2. An existing benchmark dataset provided by [Xiao et al. 2013](https://doi.org/10.1016/j.ab.2013.01.019) has been adopted by several subsequent authors but the composition of this dataset has the following issues:
    - Positive (AMP) cases in this dataset are mature peptides whereas in a genome scan only precursors sequences are usually available.
    - The negative cases in the dataset have a length distribution which suggests they are most likely full-length proteins (much longer than the positive cases). This means that predictors trained to perform well on this dataset might achieve high accuracy simply by classifying sequences into mature vs full-length proteins instead of AMP / non-AMP (the desired behaviour).
3. A realistic test of AMP prediction in genome-wide scans should use a benchmark dataset that is highly unbalanced, just as a real genome protein set would be. For example in the *Arabidopsis* genome AMPs make up less than 1% of proteins.  
4. Real genomes contain non-AMP proteins that may resemble AMPs in some ways (e.g. secreted proteins, transmembrane proteins) and which will therefore make the classification problem more difficult. Any benchmark that does not include these proteins will most likely provide inflated estimates of accuracy.

In light of these issues we tested the performance of `ampir` against contemporary AMP predictors using several benchmark datasets:

1. The [Xiao et al. 2013](https://doi.org/10.1016/j.ab.2013.01.019) benchmark dataset. This was included in the interest of consistency with benchmarking from previous work but results from this benchmark are not likely to reflect real-world performance.
2. A subset of the `ampir` mature peptide training data which we set aside for model evaluation and was not used in training. This dataset consists of known AMP mature peptides as positive cases and non-AMP mature peptides as negative cases. It should reflect real-world performance in situations where a researcher has access to a mature peptide sequence (e.g. by mass spectrometry) and wishes to determine if it is an AMP or another type of small peptide such as a toxin or neuropeptide.
3. A whole-genome scanning benchmark for species with the best available annotated AMP repertoires. We chose an animal (Human) and a plant (*Arabidopsis thaliana*) for this test. 

**Table 5.1:** AMP predictors with their papers and model accessiblity

| AMP predictor name | Reference | Availability |
| ------------------------ | --------------- | -------------- |
| AMP scanner v2 | [Veltri et al. 2018](https://doi.org/10.1093/bioinformatics/bty179) | [amp scanner webserver](https://www.dveltri.com/ascan/v2/ascan.html) |
| amPEP  | [Bhadra et al. 2018](https://doi.org/10.1038/s41598-018-19752-w) | [MATLAB source code](https://sourceforge.net/projects/axpep/files/AmPEP_MATLAB_code/)
| iAMPpred | [Meher et al. 2017](https://doi.org/10.1038/srep42362) | [iAMPpred webserver](http://cabgrid.res.in:8080/amppred/)

AMP predictors were accessed in ***April 2020***

### Mature peptide benchmarks

Both the [Xiao et al. 2013](https://doi.org/10.1016/j.ab.2013.01.019) benchmark and the `ampir` testing set are focussed on mature peptide prediction (since mature peptides form the bulk of positive cases). The benchmarks differ most substantially in the composition of their background datasets. The Xiao et al. background data has a peak in length distribution around 80-90 AA whereas for the `ampir` test set this is more similar to the target set at around 40 AA.

The plot below shows performance of all predictors in the form of receiver operating characteristic (ROC) curves. These show the tradeoff between false positive rate and true positive rate (also called recall). A few points to note from this plot:

- The `ampir_mature` model performs well on both datasets whereas the `ampir_precursor` model performs very poorly. Users of `ampir` should therefore take care to always select the appropriate model for their task depending on the nature of the input data (mature peptides or precursor proteins).
- Some predictors do not perform well at the extremes of the ROC curve. This reflects the ability of the predictor to produce accurate probability values across the full range of probabilities. In the case of `ampscanner v2` for example we see that its curve does not extend into the low false positive regime.  This is because its probability distribution is strongly concentrated at the extremes (0 and 1), and a relatively large number of non-AMP peptides have been assigned a probability of 1. 
- The best performing predictors in the low false positive regime are `ampep` and `ampir`.

```{r}
calc_cm_metrics <- function(p_threshold, df) {
  
  TP <- df %>% filter((actual=="Tg")) %>% filter(prob_AMP >= p_threshold) %>% n_distinct()
  FP <- df %>% filter((actual=="Bg")) %>% filter(prob_AMP >= p_threshold) %>% n_distinct()
  TN <- df %>% filter((actual=="Bg")) %>% filter(prob_AMP < p_threshold) %>% n_distinct()
  FN <- df %>% filter((actual=="Tg")) %>% filter(prob_AMP < p_threshold) %>% n_distinct()
  
  Specificity <- round(TN / (TN + FP), digits = 5) #aka TNR
  Recall <- round(TP / (TP + FN), digits = 5) # aka sensitivity, TPR
  Precision <- round(TP/ (TP + FP), digits = 5) # positive predictive value
  FPR <- FP / (TN + FP)
  
  cm <- c(TP, FP, TN, FN, Specificity, Recall, Precision, FPR, p_threshold)
  names(cm) <-c("TP", "FP", "TN", "FN", "Specificity", "Recall", "Precision", "FPR", "p_threshold") 
  cm
}

compress_model <- function(model){
  model$trainingData <- model$trainingData[1,]
  model$resampledCM <- NULL
  model$control <- NULL
  model
}
```



```{r, echo = FALSE}
# ampir
#
precursor_model <- compress_model(readRDS("cache/tuned_precursor_imbal_nobench.rds"))
mature_model <- compress_model(readRDS("cache/tuned_mature.rds"))

xbench <- read_faa("raw_data/benchmarking/datasets/iamp2l/iamp2l_bench.fasta")
ampir_bench <- read_faa("raw_data/benchmarking/datasets/ampir/mature_eval.fasta")
ampir_prec_bench <- read_faa("raw_data/benchmarking/datasets/ampir/precursor_eval.fasta")

predict_bench <- function(cachename,benchdata){
  if ( file.exists(cachename) ){
    xbench_ampir_raw <- read_rds(cachename)
  } else {
    xbench_ampir_raw_prec <- predict_amps(benchdata,n_cores = 4, model = precursor_model) %>% add_column(model = "ampir_precursor")
    xbench_ampir_raw_mat <- predict_amps(benchdata,n_cores = 4, model = mature_model) %>% add_column(model = "ampir_mature")
    xbench_ampir_raw <- rbind(xbench_ampir_raw_mat,xbench_ampir_raw_prec)
    write_rds(xbench_ampir_raw,cachename)
  } 
  xbench_ampir_raw
}

xbench_ampir_raw <- predict_bench("cache/xbench_ampir_raw.rds",xbench)
ampir_bench_raw <- predict_bench("cache/ampir_bench_raw.rds",ampir_bench)
ampir_prec_bench_raw <- predict_bench("cache/ampir_prec_bench_raw.rds",ampir_bench)

xbench_ampir <- xbench_ampir_raw %>% 
  mutate(actual = ifelse(grepl(seq_name,pattern = "^AP"), "Tg", "Bg")) %>% 
  mutate(predicted = ifelse(prob_AMP>0.5, "Tg","Bg"))

ampir_bench <- ampir_bench_raw %>% 
  mutate(actual = ifelse(grepl(seq_name,pattern = "_Tg"), "Tg", "Bg")) %>% 
  mutate(predicted = ifelse(prob_AMP>0.5, "Tg","Bg"))

ampir_prec_bench <- ampir_prec_bench_raw %>% 
  mutate(actual = ifelse(grepl(seq_name,pattern = "_Tg"), "Tg", "Bg")) %>% 
  mutate(predicted = ifelse(prob_AMP>0.5, "Tg","Bg"))

ampir_roc <- do.call(rbind,lapply(c("ampir_precursor","ampir_mature"),function(mdl){
  as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_ampir %>% filter(model==mdl)))) %>%
  add_column(model = mdl)
})) %>% add_column(benchmark = "Xiao")

ampir_bench_roc <- do.call(rbind,lapply(c("ampir_precursor","ampir_mature"),function(mdl){
  as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, ampir_bench %>% filter(model==mdl)))) %>%
  add_column(model = mdl)
})) %>% add_column(benchmark = "ampir")
```



```{r}
# ampep
xbench_ampep <- read_csv("raw_data/benchmarking/results/ampep/ampep_iamp2l_bench.txt") %>%
  mutate(actual = ifelse(grepl(Row,pattern = "^AP"), "Tg", "Bg")) %>% 
  rename(prob_AMP = score) %>%
  mutate(predicted = ifelse(prediction == 1, "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

ampir_bench_ampep <- read_csv("raw_data/benchmarking/results/ampep/mature_eval_ampep.txt") %>% 
  mutate(actual = ifelse(grepl(Row,pattern = "^AP"), "Tg", "Bg")) %>% 
  rename(prob_AMP = score) %>% 
  mutate(predicted = ifelse(prediction == 1, "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

ampir_prec_bench_ampep <- read_csv("raw_data/benchmarking/results/ampep/precursor_eval_ampep.txt") %>% 
  mutate(actual = ifelse(grepl(Row,pattern = "^AP"), "Tg", "Bg")) %>% 
  rename(prob_AMP = score) %>% 
  mutate(predicted = ifelse(prediction == 1, "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

ampep_roc <- as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_ampep))) %>%
  add_column(model = "ampep") %>% add_column(benchmark = "Xiao")

ampep_roc_ampir <- as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, ampir_bench_ampep))) %>%
  add_column(model = "ampep") %>% add_column(benchmark = "ampir")
```

```{r}
# ampscannerv2
xbench_ampscanv2 <- read_csv("raw_data/benchmarking/results/ampscanv2/iamp2l/1585811335833_Prediction_Summary.csv") %>% 
  mutate(actual = ifelse(grepl(SeqID,pattern = "^AP"), "Tg", "Bg")) %>% 
  rename(prob_AMP = Prediction_Probability) %>%
  mutate(predicted = ifelse(Prediction_Class == "AMP", "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))
  

ampir_bench_ampscanv2 <- read_csv("raw_data/benchmarking/results/ampscanv2/ampir/1588056261666_Prediction_Summary.csv") %>% 
  mutate(actual = ifelse(grepl(SeqID,pattern = "^AP"), "Tg", "Bg")) %>% 
  rename(prob_AMP = Prediction_Probability) %>%
  mutate(predicted = ifelse(Prediction_Class == "AMP", "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

ampir_prec_bench_ampscanv2 <- read_csv("raw_data/benchmarking/results/ampscanv2/ampir_prec/1588630046504_Prediction_Summary.csv") %>% 
  mutate(actual = ifelse(grepl(SeqID,pattern = "^AP"), "Tg", "Bg")) %>% 
  rename(prob_AMP = Prediction_Probability) %>%
  mutate(predicted = ifelse(Prediction_Class == "AMP", "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

ampscan_roc <- as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_ampscanv2))) %>%
  add_column(model = "ampscannerv2") %>% add_column(benchmark = "Xiao")

ampscan_roc_ampir <- as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, ampir_bench_ampscanv2))) %>%
  add_column(model = "ampscannerv2") %>% add_column(benchmark = "ampir")
```


```{r}
xbench_iampred <- read_csv("raw_data/benchmarking/results/iamppred/iamp2l_bench.csv") %>% 
  mutate(actual = ifelse(grepl(name_fasta,pattern = "^AP"), "Tg", "Bg")) %>% 
  mutate(prob_AMP = pmax(antibacterial,antiviral,antifungal)) %>%
  mutate(predicted = ifelse(prob_AMP >= 0.5, "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

ampir_bench_iampred <- read_tsv("raw_data/benchmarking/results/iamppred/ampir_mature.csv") %>% 
    mutate(actual = ifelse(grepl(name_fasta,pattern = "^AP"), "Tg", "Bg")) %>% 
  mutate(prob_AMP = pmax(antibacterial,antiviral,antifungal)) %>%
  mutate(predicted = ifelse(prob_AMP >= 0.5, "Tg", "Bg")) %>%
  mutate(actual = factor(actual), predicted = factor(predicted))

# TODO: ampir_prec_bench_iampred yet

iampred_roc <- as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, xbench_iampred))) %>%
  add_column(model = "iamppred") %>% add_column(benchmark = "Xiao")

iampred_roc_ampir <- as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics, ampir_bench_iampred))) %>%
  add_column(model = "iamppred") %>% add_column(benchmark = "ampir")
```


```{r}
models_roc <- rbind(ampir_roc,ampir_bench_roc, 
                    ampep_roc,ampep_roc_ampir,
                    ampscan_roc,ampscan_roc_ampir,
                    iampred_roc,iampred_roc_ampir)
```


```{r}
ggplot(models_roc) + 
  geom_line(aes(x = FPR, y = Recall, colour = model)) + 
  xlim(0,1) + facet_wrap(~benchmark) +
  labs(x = "False positive rate", y = "True positive rate")
```

**Figure 5.1:** Performance of a range of AMP predictors against two mature peptide testing datasets.  

### Real Genome Benchmark

Since we are building a model for the purpose of genome-wide prediction a realistic test must involve data with composition similar to that of a complete proteome. 

One approach is to use whole genomes that have been well annotated for AMPs. Here we chose the Human and *Arabidopsis* genomes because these represent phylogenetically distinct lineages (animals and plants), and their genomes are among the best annotated for AMPs. A few other points to note about this test are:

- We were able to run this test for `ampir`, `ampep` and `ampscanner v2` only because other predictors were unable to handle the large number of candidates sequences (~100k) in a practical manner. 
- We used a specially generated model for `ampir` that was trained without Human or *Arabidopsis* proteins to avoid any potential for overfitting resulting in inflacted accuracy estimates in this test. It should be noted that other predictors would have no such restriction.


```{r}
human_proteome <- readxl::read_excel("raw_data/benchmarking/datasets/human/uniprot-proteome_UP000005640.xlsx",guess_max = 10000)
arath_proteome <- readxl::read_excel("raw_data/benchmarking/datasets/arath/uniprot-proteome_up000006548.xlsx",guess_max = 40000)

reference_proteomes <- rbind(human_proteome, arath_proteome) %>% 
  mutate(actual = ifelse(grepl(`Keyword ID`,pattern="KW-0929"),"Tg","Bg"))
```


```{r}
# ampir

ref_df <- reference_proteomes %>% select(seq_name=Entry,seq_aa=Sequence)

if ( file.exists("cache/ref_predictions_ampir.rds")){
  ref_predictions_ampir <- read_rds("cache/ref_predictions_ampir.rds")
} else {
  ampir_genome_model <- compress_model(readRDS("cache/tuned_precursor_imbal_nobench.rds"))

  ref_predictions_ampir_prec <- predict_amps(as.data.frame(ref_df), n_cores=4, model = ampir_genome_model) %>% add_column(method="ampir_precursor")
  ref_predictions_ampir_mature <- predict_amps(as.data.frame(ref_df), n_cores=4, model = mature_model) %>% add_column(method="ampir_mature")
  
  ref_predictions_ampir <- rbind(ref_predictions_ampir_prec,ref_predictions_ampir_mature)
  
  write_rds(ref_predictions_ampir,"cache/ref_predictions_ampir.rds")
}


ampir_genome_bench <- reference_proteomes %>% left_join(ref_predictions_ampir %>% select(-seq_aa),by=c("Entry"="seq_name")) %>% 
  filter(!is.na(prob_AMP)) %>% 
  select(ID=Entry,prob_AMP,Organism,actual,method) 

organisms = c("Homo sapiens (Human)","Arabidopsis thaliana (Mouse-ear cress)")

get_genome_roc <- function(data, name){
  do.call(rbind,lapply(organisms,function(org){ 
    as.data.frame(t(sapply(c(seq(0.01, 0.99, 0.01),seq(0.99, 0.990, 0.001)), calc_cm_metrics , data %>% filter(Organism==org)))) %>%
    add_column(organism=org)
  })) %>%   
  add_column(model = name)
}

if ( file.exists("cache/ampir_genome_roc.rds")){
  ampir_genome_roc <- read_rds("cache/ampir_genome_roc.rds")
} else {
  ampir_genome_roc <- do.call(rbind,lapply(c("ampir_precursor","ampir_mature"),function(meth){
    get_genome_roc(ampir_genome_bench %>% filter(method==meth),meth)
  }))
  write_rds(ampir_genome_roc,"cache/ampir_genome_roc.rds")
}
```



```{r}
ampscan_files <- c(list.files("raw_data/benchmarking/results/ampscanv2/arath/", pattern="*.csv",full.names = T),
                   list.files("raw_data/benchmarking/results/ampscanv2/human/", pattern="*.csv",full.names = T))

ampscan_genome_bench <- do.call(rbind,lapply(ampscan_files,read_csv)) %>% 
  separate(SeqID,into = c("database","Entry","Entry name"),sep = "\\|") %>% 
  left_join(reference_proteomes,by="Entry") %>% 
  select(ID=Entry,prob_AMP=Prediction_Probability,Organism,actual) %>% 
  add_column(method="ampscannerv2")

ampscan_genome_roc <- get_genome_roc(ampscan_genome_bench,"ampscannerv2")
```


```{r}
ampep_files <- c("raw_data/benchmarking/results/ampep//arath_ampep.txt","raw_data/benchmarking/results/ampep//human_ampep.txt")

ampep_genome_bench <- do.call(rbind,lapply(ampep_files,read_csv)) %>% 
  separate(Row,into = c("database","Entry","Entry name"),sep = "\\|") %>% 
  left_join(reference_proteomes,by="Entry") %>% 
  select(ID=Entry,prob_AMP=score,Organism,actual) %>% 
  add_column(method="ampep")

ampep_genome_roc <- get_genome_roc(ampep_genome_bench,"ampep")
```

```{r}
genome_rocs <- rbind(ampir_genome_roc,ampscan_genome_roc,ampep_genome_roc)

ggplot(genome_rocs) + 
  geom_line(aes(x = FPR, y = Recall, colour = model)) + 
  xlim(0,1) +
  facet_wrap(~organism) +
  labs(x= "False positive rate", y = "True positive rate")
```

**Figure 5.2:** Performance of various AMP predictors in classifying whole proteome data for Human and *Arabidopsis*. 

In Figure 5.2 we show ROC curves for all predictors on the Human and *Arabidopsis* data but it is important to remember that in this context the low false positive regime is especially important. This is because of the extremely low frequency of true positives in the data (less than 1%). This is explored further in Figure 5.3 but for now it is important to note that `ampscanner v2` is not shown in Figure 5.3 because its ROC curve does not extend into this important regime despite the fact that it otherwise appears to perform very well. `AmPEP` and `ampir_mature` both perform very poorly reflecting the emphasis of their training data on mature peptides rather than precursor proteins.

In order to properly capture the real-world performance of predictors on genome scans it is important to use a plot that emphasises the absolute numbers of true and false positives. On this measure (shown in Figure 5.3) it can be seen that genome-wide prediction of AMPs is still an imperfectly solved problem.  Although the `ampir` precursor model clearly performs far better than any other predictors, none were able to predict more than 50% of true AMPs while controlling false positives to under 500. Nevertheless, given the difficulties in identifying AMPs and the importance of this task this level of enrichment is of great practical use, reducing the number of false experimental leads per true positive from many thousands down to tens or hundreds. 

```{r, fig.width=8, fig.height=3}
library(RColorBrewer)
models <- c("ampep","ampir_mature","ampir_precursor","ampscannerv2","iamppred")
model_colors <- brewer.pal(5,"Set1")
names(model_colors) <- models
model_names <- c("amPEP","ampir mature","ampir precursor","AMP Scanner v2","iAMPPred")
names(model_names) <- models

library(ggpubr)
pg <- genome_rocs %>% mutate(organism = ifelse(organism=="Homo sapiens (Human)","Human Genome","Arabidopsis Genome")) %>% 
  ggplot() + 
  geom_line(aes(x = FP, y = TP, colour = model),size=1.1) + 
  xlim(0,500) +
  scale_color_manual(values = model_colors) +
  labs(x = "False Positives", y = "True Positives") +
  facet_wrap(~organism, scales = "free_y", nrow = 1) + 
  theme_pubr() + theme(legend.position = "None") +
  theme(strip.background = element_blank(), strip.placement = "outside", strip.text = element_text(size = 10)) +
  theme(axis.text = element_text(size=8))

pp <- ggplot(models_roc %>% filter(benchmark=="ampir")) + 
  geom_line(aes(x = FPR, y = Recall, colour = model),size=1.1) + 
  labs(x= "False positive rate", y = "True positive rate") +
  scale_color_manual(values = model_colors, labels = model_names, name="") +
    xlim(0,1) +
  theme_pubr() + theme(legend.position = "None") + ggtitle("Mature Peptides") +
  theme(plot.title = element_text(size=10, hjust = 0.5, vjust = -2))+
  theme(axis.text = element_text(size=10))

legend_b <- get_legend(
  pp + 
    guides(color = guide_legend(nrow = 1)) +
    theme(legend.position = "bottom")
)

library(cowplot)
prow <- plot_grid(pg,pp, rel_widths = c(2,1))
plot_grid(prow,legend_b, ncol = 1,rel_heights = c(1,0.1))
ggsave("figures/benchmarks.png", width = 178,units = "mm", height = 80,dpi = 600)
ggsave("figures/benchmarks.eps", width = 178,units = "mm", height = 80,dpi = 600)
```
**Figure 5.3:** Performance of `ampir` compared with three existing AMP prediction models iAMPpred (Meher et al., 2017), AmPEP (Bhadra et al., 2018), AMP Scanner (Veltri et al., 2018). Results for iAMPpred are not shown for parts A and B because it was impractical to run on large numbers of sequences. Parts A and B are scaled so that the limits of the y-axis show the full complement of known AMPs in each genome (291 for *Arabidopsis*, 101 for Human), and the limits of the x-axis are restricted to emphasise behaviour in the low false positive (FP) regime (FP < 500) because this is most relevant in whole genome scans. Part C is a receiver operating characteristic (ROC) curve based on the `ampir` reserved testing data. It shows FPR (False Positive Rate) versus Recall (True Positive Rate).



## Performance Statistics

```{r}
get_performance_stats <- function(test_results){
  x_cm_prec <- confusionMatrix(factor(test_results$predicted), factor(test_results$actual), positive = "Tg")
  
  xrc_prec <- roc(test_results$actual, test_results$prob_AMP) 
  c(x_cm_prec$byClass[c(11,5:7)],AUC=as.numeric(xrc_prec$auc))
}

# ampir
xbench_ampir_prec <- filter(xbench_ampir, model == "ampir_precursor")
xbench_ampir_mature <- filter(xbench_ampir, model == "ampir_mature")

xbench_data <- list(ampir_mature=xbench_ampir_mature,
                    ampir_precursor=xbench_ampir_prec,
                    ampscannerv2=xbench_ampscanv2,
                    ampep=xbench_ampep,
                    iamppred=xbench_iampred)

xbench_stats <- sapply(xbench_data,get_performance_stats)
```


```{r}
ampir_bench_data <- list(ampir_mature = ampir_bench %>% filter(model=="ampir_mature"),
                         ampir_precursor = ampir_bench %>% filter(model=="ampir_precursor"),
                         ampscannerv2=ampir_bench_ampscanv2,
                         ampep=ampir_bench_ampep,
                         iamppred = ampir_bench_iampred)
ampir_bench_stats <- sapply(ampir_bench_data,get_performance_stats)
```

```{r}
ampir_prec_bench_data <- list(ampir_mature = ampir_prec_bench %>% filter(model=="ampir_mature"),
                         ampir_precursor = ampir_prec_bench %>% filter(model=="ampir_precursor"),
                         ampscannerv2=ampir_prec_bench_ampscanv2,
                         ampep=ampir_prec_bench_ampep)
ampir_prec_bench_stats <- sapply(ampir_bench_data,get_performance_stats)
```


**Table 5.2:** Model performance on Xiao et al. benchmark dataset
```{r}
knitr::kable(xbench_stats, digits = 2)
```


**Table 5.3:** Model performance on `ampir_mature` test set
```{r}
knitr::kable(ampir_bench_stats, digits = 2)
```


**Table 5.4:** Model performance on `ampir_precursor` test set
```{r}
knitr::kable(ampir_prec_bench_stats, digits = 2)
```
