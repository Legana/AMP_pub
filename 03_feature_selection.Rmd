---
title: "Feature selection"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, eval = TRUE)
```

```{r, echo=FALSE}
library(caret)
library(tidyverse)
set.seed(10)
```

## Feature distributions

As an indication of features that are likely to be useful for classification we plot their distributions for background and target.  Also note that these feature distributions are sometimes heavily influenced by background filtering.  In the plot below we see that all distributions are relatively well-behaved and should be amenable to centering and scaling.  This is at least partly because our large protein cut-off of (500) removes a small number of very large proteins that cause skew in the Mw and Charge distributions.

```{r, fig.height=20}
features_tg_bg_1 <- read_rds("cache/features_precursor.rds")
features_bg_tg_1_long <- features_tg_bg_1 %>% gather(key = "Feature","Value",-seq_name,-Label) %>% add_column(database="precursors")

features_tg_bg_mature <- read_rds("cache/features_mature.rds")
features_bg_tg_mature_long <- features_tg_bg_mature %>% gather(key = "Feature","Value",-seq_name,-Label) %>% add_column(database="mature")

features_long <- rbind(features_bg_tg_1_long,features_bg_tg_mature_long)

ggplot(features_bg_tg_1_long,aes(x=Value)) + 
  geom_density(aes(color=Label)) + 
  facet_wrap(~Feature, scales = "free", ncol = 3)
```


## PCA

PCA suggests that these predictors have some (but imperfect) power to separate the two classes.  This gives an indication of how well models will perform in general but doesn't capture the capabilities of supervised learning methods like SVM.

```{r}
pca_features_1 <- features_tg_bg_1[,-c(1,46)]
rownames(pca_features_1) <- features_tg_bg_1$seq_name
pca_1 <- prcomp(pca_features_1, scale. = TRUE)
pca_1_plot <- pca_1$x %>% as.data.frame() %>%  rownames_to_column("seq_name") %>% left_join(features_tg_bg_1)

p_sc <- ggplot(pca_1_plot,aes(x=PC1,y=PC2)) + geom_point(aes(color=Label)) + theme(legend.position = "none")
p_xd <- ggplot(pca_1_plot,aes(x=PC1)) + geom_density(aes(color=Label))
p_yd <- ggplot(pca_1_plot,aes(x=PC2)) + geom_density(aes(color=Label)) + coord_flip()

library(cowplot)

# Remove some duplicate axes
p_xd = p_xd + theme(axis.title.x=element_blank(),
				axis.text=element_blank(),
				axis.line=element_blank(),
				axis.ticks=element_blank(),
				legend.position = "none")

p_yd = p_yd + theme(axis.title.y=element_blank(),
				axis.text=element_blank(),
				axis.line=element_blank(),
				axis.ticks=element_blank())

# Modify margin c(top, right, bottom, left) to reduce the distance between plots
#and align G1 density with the scatterplot
p_xd = p_xd + theme(plot.margin = unit(c(0.5, 0, 0, 0.7), "cm"))
p_sc = p_sc + theme(plot.margin = unit(c(0, 0, 0.5, 0.5), "cm"))
p_yd = p_yd + theme(plot.margin = unit(c(0, 0.5, 0.5, 0), "cm"))

# Combine all plots together and crush graph density with rel_heights
first_col = plot_grid(p_xd, p_sc, ncol = 1, rel_heights = c(1, 3))
second_col = plot_grid(NULL, p_yd, ncol = 1, rel_heights = c(1, 3))

plot_grid(first_col, second_col, ncol = 2, rel_widths = c(3, 1))
```



```{r, fig.height=18}
pca_1$rotation %>% as.data.frame() %>% rownames_to_column("Feature") %>% ggplot(aes(x=Feature,y=PC1)) + geom_col() + coord_flip()
```


## Correlated Predictors

Although a small number these predictors are correlated there are none with near-perfect correlation (max cor < 0.9).  We therefore did not remove any features on the basis of correlation since this is unlikely to negatively affect model performance. 

```{r}
featuresCM <- cor(features_tg_bg_1[,-c(1,46)])
summary(featuresCM[upper.tri(featuresCM)])
```

## Recursive feature elimination (RFE)

RFE analysis was used to find an optimal subset of the features to be included in the model. Since this is a computationally intensive process it was performed using the `rfe.R` and `rfe.sh` scripts on an HPC system.  The resulting `rfe` outputs suggest that the best performance can be obtained with 20-30 predictors.  

The set of features identified as optimal by rfe includes all bulk physicochemical properties as well as most simple amino acid composition measures.  Higher order pseudoamino-acid composition measures do not appear to be important to model performance according RFE.

```{r}
svmProfile <- readRDS("cache/rfe_precursor.rds")
trellis.par.set(caretTheme())
plot(svmProfile, type = c("g", "o"))
```

```{r}
predictors(svmProfile)
```


