---
title: "Feature selection"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, eval = TRUE, warning = FALSE)
```

```{r, echo=FALSE}
library(caret)
library(tidyverse)
set.seed(10)
```

As an indication of features that are likely to be useful for classification we plot their distributions for background and target. Also note that these feature distributions are sometimes heavily influenced by background filtering. In the plots below we see that all distributions are relatively well-behaved and should be amenable to centering and scaling. This is at least partly because our large protein cut-off of (500 amino acids) removes a small number of very large proteins that cause skew in the Mw and Charge distributions.  

For both sets of data it is clear that higher order lambda values from the Pseudo-amino acid composition seem to show little difference between target and background (this is not true of low order values though). For both precursor and mature peptide models we used all physicochemical predictors, all Xc1 predictors and the first two Xc2 predictors.

```{r, fig.height=20}
features_tg_bg_1 <- read_rds("raw_data/ampir_train_test/featuresTrain_precursor_imbal.rds")
features_bg_tg_1_long <- features_tg_bg_1 %>% gather(key = "Feature","Value",-seq_name,-Label) %>% add_column(database="precursors")

features_tg_bg_mature <- read_rds("raw_data/ampir_train_test/featuresTrain_mature.rds")
features_bg_tg_mature_long <- features_tg_bg_mature %>% gather(key = "Feature","Value",-seq_name,-Label) %>% add_column(database="mature")

features_long <- rbind(features_bg_tg_1_long,features_bg_tg_mature_long)

ggplot(features_long %>% filter(database=="precursors"),aes(x=Value)) + 
  geom_density(aes(color=Label)) + 
  facet_wrap(~Feature, scales = "free", ncol = 3) +
  labs(x= "Feature", y = "Density") +
  scale_color_discrete(name = "", labels = c("Background", "Target"))
```

**Figure 3.1:** Feature distributions for precursor training data


```{r, fig.height=20}
ggplot(features_long %>% filter(database=="mature"),aes(x=Value)) + 
  geom_density(aes(color=Label)) + 
  facet_wrap(~Feature, scales = "free", ncol = 3) +
  labs(x= "Feature", y = "Density") +
  scale_color_discrete(name = "", labels = c("Background", "Target"))
```

**Figure 3.2:** Feature distributions for mature peptide training data

### PCA

Principal component analysis (PCA) suggests that these predictors have some (but imperfect) power to separate the two classes. This gives an indication of how well models will perform in general but does not capture the capabilities of supervised learning methods like SVM.

```{r, message=FALSE, warning=FALSE}
pca_features_1 <- features_tg_bg_1[,-c(1,46)]
rownames(pca_features_1) <- features_tg_bg_1$seq_name
pca_1 <- prcomp(pca_features_1, scale. = TRUE)
pca_1_plot <- pca_1$x %>% as.data.frame() %>%  rownames_to_column("seq_name") %>% left_join(features_tg_bg_1)

p_sc <- ggplot(pca_1_plot,aes(x=PC1,y=PC2)) + geom_point(aes(color=Label)) + theme(legend.position = "none")
p_xd <- ggplot(pca_1_plot,aes(x=PC1)) + geom_density(aes(color=Label))
p_yd <- ggplot(pca_1_plot,aes(x=PC2)) + geom_density(aes(color=Label)) + coord_flip()  + scale_color_discrete(name = "", labels = c("Background", "Target"))

library(cowplot)

# Remove some duplicate axes
p_xd = p_xd + theme(axis.title.x=element_blank(),
				axis.text=element_blank(),
				axis.line=element_blank(),
				axis.ticks=element_blank(),
				legend.position = "none")

p_yd = p_yd + theme(axis.title.y=element_blank(),
				axis.text=element_blank(),
				axis.line=element_blank(),
				axis.ticks=element_blank())

# Modify margin c(top, right, bottom, left) to reduce the distance between plots
#and align G1 density with the scatterplot
p_xd = p_xd + theme(plot.margin = unit(c(0.5, 0, 0, 0.7), "cm"))
p_sc = p_sc + theme(plot.margin = unit(c(0, 0, 0.5, 0.5), "cm"))
p_yd = p_yd + theme(plot.margin = unit(c(0, 0.5, 0.5, 0), "cm"))

# Combine all plots together and crush graph density with rel_heights
first_col = plot_grid(p_xd, p_sc, ncol = 1, rel_heights = c(1, 3))
second_col = plot_grid(NULL, p_yd, ncol = 1, rel_heights = c(1, 3))

plot_grid(first_col, second_col, ncol = 2, rel_widths = c(3, 1))
```

**Figure 3.3:** PCA with marginal density plots showing the ability of this unsupervised clustering method to separate classes. The plot shows data from the precursor training set



## Correlated Predictors

Although a small number show these predictors are correlated there are none with near-perfect correlation (max cor < 0.9). We therefore did not remove any features on the basis of correlation since this is unlikely to negatively affect model performance. 

```{r}
featuresCM <- cor(features_tg_bg_1[,-c(1,46)])
summary(featuresCM[upper.tri(featuresCM)])
```

## Recursive feature elimination (RFE)

RFE analysis in principle can be used to find an optimal subset of the features to be included in the model. In practice we used this as a guide only. Since this is a computationally intensive process it was performed using the `rfe.R` and `rfe.sh` scripts on an HPC system. The resulting `rfe` outputs suggest that the best performance can be obtained with 20-30 predictors.  

The set of features identified as optimal by RFE includes all bulk physicochemical properties as well as most simple amino acid composition measures. Higher order pseudoamino-acid composition measures do not appear to be important to model performance according to RFE.

```{r}
svmProfile <- readRDS("raw_data/ampir_train_test/rfe_precursor.rds")
predictors(svmProfile)
```


